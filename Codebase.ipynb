{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyJDkvtqEuw8","outputId":"882f6a9a-1b5e-474f-a50e-7f2dc4dd68f1","executionInfo":{"status":"ok","timestamp":1723561678054,"user_tz":-360,"elapsed":4151,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":66,"metadata":{"id":"eLvQs3Rma7hD","collapsed":true,"executionInfo":{"status":"ok","timestamp":1723561679730,"user_tz":-360,"elapsed":7,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"outputs":[],"source":["import pandas as pd\n","file_path = '/content/drive/MyDrive/GTL_dataset/data.tsv'"]},{"cell_type":"code","source":["sentences = []\n","pos_tags = []\n","ner_tags = []\n","\n","# Temporary storage for words, POS, and NER tags of a sentence.\n","temp_sentence = []\n","temp_pos_tags = []\n","temp_ner_tags = []\n","\n","with open(file_path, 'r') as file:\n","    for line in file:\n","        line = line.strip()\n","        # Check if the line is a sentence or a word\n","        # In the given dataset, five sentences had one tab separation and rest had no tab separation\n","        tab_count = line.count('\\t')\n","        if tab_count == 0 or tab_count == 1:\n","\n","            # If it's a sentence (zero or one tabs), finalize the previous sentence\n","            if temp_sentence:\n","                sentences.append(temp_sentence)\n","                pos_tags.append(temp_pos_tags)\n","                ner_tags.append(temp_ner_tags)\n","                temp_sentence = []\n","                temp_pos_tags = []\n","                temp_ner_tags = []\n","        else:\n","            # If it's a word with tags, split by tabs\n","            parts = line.split('\\t')\n","            temp_sentence.append(parts[0])\n","            if len(parts) > 1:\n","                temp_pos_tags.append(parts[1])\n","            if len(parts) > 2:\n","                temp_ner_tags.append(parts[2])\n","\n","# Append the last sentence if the loop ends without a sentence boundary\n","if temp_sentence:\n","    sentences.append(temp_sentence)\n","    pos_tags.append(temp_pos_tags)\n","    ner_tags.append(temp_ner_tags)\n","\n","# Convert sentences, pos_tags, and ner_tags to a DataFrame for better inspection\n","processed_data = pd.DataFrame({\n","    'Sentence': [\" \".join(sentence) for sentence in sentences],\n","    'POS_Tags': pos_tags,\n","    'NER_Tags': ner_tags\n","})\n","\n","# Display the processed data\n","print(processed_data.head())\n","processed_data.POS_Tags\n","processed_data.NER_Tags.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"collapsed":true,"id":"ccD74GWKocwg","outputId":"481d9104-b48b-476c-a4c2-27e4ccaf8a63","executionInfo":{"status":"ok","timestamp":1723561771564,"user_tz":-360,"elapsed":554,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            Sentence  \\\n","0  শনিবার (২৭ আগস্ট) রাতে পটুয়াখালী সদর থানার ভা...   \n","1  বায়ুদূষণ ও স্মার্ট ফোন ছেলেমেয়ে উভয়ের প্রজনন ক...   \n","2  ছাত্র রাজনীতির বর্তমান অবস্থার শুরু হয়েছিলো ...   \n","3  শাকিল রাজধানীর ৩০০ ফিট, দিয়াবাড়ি ও পূর্বাচল ...   \n","4  সম্প্রতি ক্লাবের নবীন ব্যবস্থাপনা প্রশিক্ষণার্...   \n","\n","                                            POS_Tags  \\\n","0  [NNP, PUNCT, NNP, NNC, NNP, NNC, NNC, ADJ, NNC...   \n","1  [NNC, CONJ, NNC, NNC, NNC, PRO, NNC, NNC, NNC,...   \n","2   [NNC, NNC, ADJ, NNC, NNC, VF, NNC, NNP, NNC, PP]   \n","3  [NNP, NNC, QF, NNC, NNP, CONJ, NNP, NNC, NNC, ...   \n","4  [ADV, NNC, ADJ, NNC, NNC, CONJ, NNC, NNC, PP, ...   \n","\n","                                            NER_Tags  \n","0  [B-D&T, B-OTH, B-D&T, B-D&T, B-GPE, I-GPE, I-G...  \n","1  [B-OTH, B-OTH, B-OTH, B-OTH, B-PER, B-OTH, B-O...  \n","2  [B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-P...  \n","3  [B-PER, B-OTH, B-LOC, I-LOC, B-LOC, B-OTH, B-L...  \n","4  [B-OTH, B-ORG, B-OTH, B-OTH, B-PER, B-OTH, B-P...  \n"]},{"output_type":"execute_result","data":{"text/plain":["0    [B-D&T, B-OTH, B-D&T, B-D&T, B-GPE, I-GPE, I-G...\n","1    [B-OTH, B-OTH, B-OTH, B-OTH, B-PER, B-OTH, B-O...\n","2    [B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-P...\n","3    [B-PER, B-OTH, B-LOC, I-LOC, B-LOC, B-OTH, B-L...\n","4    [B-OTH, B-ORG, B-OTH, B-OTH, B-PER, B-OTH, B-P...\n","Name: NER_Tags, dtype: object"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NER_Tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[B-D&amp;T, B-OTH, B-D&amp;T, B-D&amp;T, B-GPE, I-GPE, I-G...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[B-OTH, B-OTH, B-OTH, B-OTH, B-PER, B-OTH, B-O...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-P...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[B-PER, B-OTH, B-LOC, I-LOC, B-LOC, B-OTH, B-L...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[B-OTH, B-ORG, B-OTH, B-OTH, B-PER, B-OTH, B-P...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["flat_pos_labels = [label for sublist in pos_tags for label in sublist]\n","flat_ner_labels = [label for sublist in ner_tags for label in sublist]\n","\n","unique_pos_classes = set(flat_pos_labels)\n","pos_classes = len(unique_pos_classes)\n","print(f\"POS classes: {unique_pos_classes}, Number of POS classes: {pos_classes}\")\n","\n","# Find unique NER classes\n","unique_ner_classes = set(flat_ner_labels)\n","ner_classes = len(unique_ner_classes)\n","print(f\"NER classes: {unique_ner_classes}, Number of NER classes: {ner_classes}\")\n"],"metadata":{"id":"vLaysnfPJshf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4285fe1-6d92-4372-e85c-595a91980f7a","executionInfo":{"status":"ok","timestamp":1723561779797,"user_tz":-360,"elapsed":506,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["POS classes: {'VNF', 'PUNCT', 'ADJ', 'VF', 'PRO', 'OTH', 'INTJ', 'NNP', 'ADV', 'NNC', 'PP', 'CONJ', 'PART', 'QF', 'DET'}, Number of POS classes: 15\n","NER classes: {'B-ORG', 'I-MISC', 'I-LOC', 'B-NUM', 'I-ORG', 'B-D&T', 'B-MISC', 'B-OTH', 'I-NUM', 'I-EVENT', 'B-T&T', 'I-UNIT', 'B-LOC', 'B-GPE', 'I-D&T', 'B-UNIT', 'I-T&T', 'B-PER', 'I-PER', 'B-EVENT', 'I-GPE'}, Number of NER classes: 21\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import numpy as np\n","# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Tokenize the sentences\n","inputs = tokenizer(sentences, padding=True, truncation=True, max_length=92, is_split_into_words=True, return_tensors='np')  # Use 'np' to return numpy arrays\n","\n","max_token_id = np.max(inputs['input_ids'])\n","vocab_size = tokenizer.vocab_size\n","\n","assert max_token_id < vocab_size, f\"Token ID {max_token_id} exceeds the vocab size of {vocab_size}. Check your tokenization process.\"\n","\n","\n","# Convert tags to label encoding\n","ner_label_encoder = LabelEncoder()\n","pos_label_encoder = LabelEncoder()\n","\n","# Flatten the list of tags, fit and transform\n","flat_ner_tags = [item for sublist in ner_tags for item in sublist]\n","flat_pos_tags = [item for sublist in pos_tags for item in sublist]\n","\n","ner_label_encoder.fit(flat_ner_tags)\n","pos_label_encoder.fit(flat_pos_tags)\n","\n","encoded_ner_tags = [ner_label_encoder.transform(tag_list) for tag_list in ner_tags]\n","encoded_pos_tags = [pos_label_encoder.transform(tag_list) for tag_list in pos_tags]\n","\n","# Pad the encoded tags to match input length\n","ner_tags_padded = tf.keras.preprocessing.sequence.pad_sequences(encoded_ner_tags, maxlen=92, padding='post', truncating='post')\n","pos_tags_padded = tf.keras.preprocessing.sequence.pad_sequences(encoded_pos_tags, maxlen=92, padding='post', truncating='post')\n","\n","# Convert to numpy arrays to ensure compatibility with scikit-learn\n","input_ids = inputs['input_ids']\n","attention_mask = inputs['attention_mask']\n","\n","# Split the data into train, validation, and test sets\n","X_train, X_temp, y_train_ner, y_temp_ner, y_train_pos, y_temp_pos = train_test_split(\n","    input_ids, ner_tags_padded, pos_tags_padded, test_size=0.3, random_state=42)\n","\n","X_val, X_test, y_val_ner, y_test_ner, y_val_pos, y_test_pos = train_test_split(\n","    X_temp, y_temp_ner, y_temp_pos, test_size=0.5, random_state=42)\n","\n","# Split attention_mask as well to match X_train, X_val, and X_test\n","attention_mask_train, attention_mask_temp = train_test_split(attention_mask, test_size=0.3, random_state=42)\n","attention_mask_val, attention_mask_test = train_test_split(attention_mask_temp, test_size=0.5, random_state=42)\n","\n","# Convert to tf.Tensor to feed into the model\n","X_train = tf.convert_to_tensor(X_train)\n","X_val = tf.convert_to_tensor(X_val)\n","X_test = tf.convert_to_tensor(X_test)\n","\n","y_train_ner = tf.convert_to_tensor(y_train_ner)\n","y_val_ner = tf.convert_to_tensor(y_val_ner)\n","y_test_ner = tf.convert_to_tensor(y_test_ner)\n","\n","y_train_pos = tf.convert_to_tensor(y_train_pos)\n","y_val_pos = tf.convert_to_tensor(y_val_pos)\n","y_test_pos = tf.convert_to_tensor(y_test_pos)\n","\n","attention_mask_train = tf.convert_to_tensor(attention_mask_train)\n","attention_mask_val = tf.convert_to_tensor(attention_mask_val)\n","attention_mask_test = tf.convert_to_tensor(attention_mask_test)\n"],"metadata":{"id":"yQpGiSeoLYf8","executionInfo":{"status":"ok","timestamp":1723561821960,"user_tz":-360,"elapsed":8277,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["def pad_sequences(sequences, maxlen):\n","    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n","\n","# Define maximum sequence length\n","max_seq_length = 92\n","\n","# Pad sequences and labels\n","X_train_padded = pad_sequences(X_train, maxlen=max_seq_length)\n","y_train_ner_padded = pad_sequences(y_train_ner, maxlen=max_seq_length)\n","y_train_pos_padded = pad_sequences(y_train_pos, maxlen=max_seq_length)\n","\n","X_val_padded = pad_sequences(X_val, maxlen=max_seq_length)\n","y_val_ner_padded = pad_sequences(y_val_ner, maxlen=max_seq_length)\n","y_val_pos_padded = pad_sequences(y_val_pos, maxlen=max_seq_length)\n","\n","# Convert to tensors\n","X_train_tensor = tf.convert_to_tensor(X_train_padded)\n","y_train_ner_tensor = tf.convert_to_tensor(y_train_ner_padded)\n","y_train_pos_tensor = tf.convert_to_tensor(y_train_pos_padded)\n","\n","X_val_tensor = tf.convert_to_tensor(X_val_padded)\n","y_val_ner_tensor = tf.convert_to_tensor(y_val_ner_padded)\n","y_val_pos_tensor = tf.convert_to_tensor(y_val_pos_padded)\n","\n","# Example with attention masks (all ones for simplicity; adjust as needed)\n","attention_mask_train = np.ones_like(X_train_tensor)\n","attention_mask_val = np.ones_like(X_val_tensor)\n"],"metadata":{"collapsed":true,"id":"qsn9R5gcWhuD","executionInfo":{"status":"ok","timestamp":1723561870520,"user_tz":-360,"elapsed":19453,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["print('Input IDs shape:', input_ids.shape)  # Should be (batch_size, sequence_length)\n","print('Attention Mask shape:', attention_mask.shape)\n","print(max_token_id)\n","vocab_size\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QcWWJVKGGaIl","outputId":"d57a7011-603a-4f30-d262-44d0e9d816dd","executionInfo":{"status":"ok","timestamp":1723561942398,"user_tz":-360,"elapsed":548,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs shape: (7002, 92)\n","Attention Mask shape: (7002, 92)\n","111240\n"]},{"output_type":"execute_result","data":{"text/plain":["119547"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["from transformers import TFBertModel\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model\n","import tensorflow as tf\n"],"metadata":{"id":"J725yVX4G40a","executionInfo":{"status":"ok","timestamp":1723562085172,"user_tz":-360,"elapsed":480,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["class BertMultiTaskModel(Model):\n","    def __init__(self, hidden_dim, ner_classes, pos_classes, **kwargs):\n","        super(BertMultiTaskModel, self).__init__(**kwargs)\n","        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.hidden_dim = hidden_dim\n","        self.ner_classes = ner_classes\n","        self.pos_classes = pos_classes\n","        self.dense = Dense(hidden_dim, activation='relu')\n","        self.ner_output = Dense(ner_classes, activation='softmax', name='ner_output')\n","        self.pos_output = Dense(pos_classes, activation='softmax', name='pos_output')\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        # BERT embeddings\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","\n","        # Apply dense layer and separate outputs\n","        x = self.dense(sequence_output)\n","        ner_logits = self.ner_output(x)\n","        pos_logits = self.pos_output(x)\n","\n","        return {'ner_output': ner_logits, 'pos_output': pos_logits}\n","\n","    def get_config(self):\n","        return {\n","            'hidden_dim': self.hidden_dim,\n","            'ner_classes': self.ner_classes,\n","            'pos_classes': self.pos_classes\n","        }\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","    def build_from_config(self, config):\n","        self.__init__(**config)\n","        self.build((None, 92))  # Example input shape; adjust as needed\n","\n","hidden_dim = 256\n","class BertMultiTaskModel(Model):\n","    def __init__(self, hidden_dim, ner_classes, pos_classes, **kwargs):\n","        super(BertMultiTaskModel, self).__init__(**kwargs)\n","        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.hidden_dim = hidden_dim\n","        self.ner_classes = ner_classes\n","        self.pos_classes = pos_classes\n","        self.dense = Dense(hidden_dim, activation='relu')\n","        self.ner_output = Dense(ner_classes, activation='softmax', name='ner_output')\n","        self.pos_output = Dense(pos_classes, activation='softmax', name='pos_output')\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        # BERT embeddings\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","\n","        # Apply dense layer and separate outputs\n","        x = self.dense(sequence_output)\n","        ner_logits = self.ner_output(x)\n","        pos_logits = self.pos_output(x)\n","\n","        return {'ner_output': ner_logits, 'pos_output': pos_logits}\n","\n","    def get_config(self):\n","        return {\n","            'hidden_dim': self.hidden_dim,\n","            'ner_classes': self.ner_classes,\n","            'pos_classes': self.pos_classes\n","        }\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","    def build_from_config(self, config):\n","        self.__init__(**config)\n","        self.build((None, 92))  # Example input shape; adjust as needed\n","\n","hidden_dim = 256\n","ner_classes = 21  # Number of NER classes\n","pos_classes = 15  # Number of POS classes\n","learning_rate = 0.0001\n","\n","input_ids = Input(shape=(92,), dtype=tf.int32, name='input_ids')\n","attention_mask = Input(shape=(92,), dtype=tf.int32, name='attention_mask')\n","\n","model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","model({'input_ids': input_ids, 'attention_mask': attention_mask})  # This will build the model\n","\n","# Compile the model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","    loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","    metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'}\n",")\n","model.summary()\n","learning_rate = 0.0001\n","\n","input_ids = Input(shape=(92,), dtype=tf.int32, name='input_ids')\n","attention_mask = Input(shape=(92,), dtype=tf.int32, name='attention_mask')\n","\n","model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","model({'input_ids': input_ids, 'attention_mask': attention_mask})  # This will build the model\n","\n","# Compile the model\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","    loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","    metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'}\n",")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"id":"-U4tzhT4s4H9","outputId":"3db0fea5-0b97-4b8f-9568-3f580ef9f5cc","executionInfo":{"status":"ok","timestamp":1723562103307,"user_tz":-360,"elapsed":6812,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":74,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"bert_multi_task_model_23\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_multi_task_model_23\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m196,864\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m21\u001b[0m)              │           \u001b[38;5;34m5,397\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m92\u001b[0m, \u001b[38;5;34m15\u001b[0m)              │           \u001b[38;5;34m3,855\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,397</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">92</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m206,116\u001b[0m (805.14 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">206,116</span> (805.14 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m206,116\u001b[0m (805.14 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">206,116</span> (805.14 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Train the model\n","from tensorflow.keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',           # Metric to monitor\n","    patience=3,                   # Number of epochs with no improvement to wait\n","    restore_best_weights=True,    # Restore model weights from the epoch with the best value of the monitored quantity\n","    verbose=1                     # Verbosity mode\n",")\n","\n","history = model.fit(\n","    {'input_ids': X_train_tensor, 'attention_mask': attention_mask_train},\n","    {'ner_output': y_train_ner_tensor, 'pos_output': y_train_pos_tensor},\n","    validation_data=(\n","        {'input_ids': X_val_tensor, 'attention_mask': attention_mask_val},\n","        {'ner_output': y_val_ner_tensor, 'pos_output': y_val_pos_tensor}\n","    ),\n","    epochs=30,\n","    batch_size=32,\n","    callbacks=[early_stopping]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4InnsQXasXK","outputId":"9ef7d3c1-6fb3-41a7-8bfb-bfb044e91314","executionInfo":{"status":"ok","timestamp":1723564216023,"user_tz":-360,"elapsed":1237416,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 261ms/step - loss: 0.6302 - ner_output_accuracy: 0.9178 - pos_output_accuracy: 0.9002 - val_loss: 0.6137 - val_ner_output_accuracy: 0.9192 - val_pos_output_accuracy: 0.9008\n","Epoch 2/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 183ms/step - loss: 0.5973 - ner_output_accuracy: 0.9228 - pos_output_accuracy: 0.9031 - val_loss: 0.5880 - val_ner_output_accuracy: 0.9248 - val_pos_output_accuracy: 0.9025\n","Epoch 3/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 185ms/step - loss: 0.5693 - ner_output_accuracy: 0.9262 - pos_output_accuracy: 0.9065 - val_loss: 0.5699 - val_ner_output_accuracy: 0.9274 - val_pos_output_accuracy: 0.9048\n","Epoch 4/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 185ms/step - loss: 0.5519 - ner_output_accuracy: 0.9288 - pos_output_accuracy: 0.9075 - val_loss: 0.5574 - val_ner_output_accuracy: 0.9295 - val_pos_output_accuracy: 0.9069\n","Epoch 5/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 225ms/step - loss: 0.5388 - ner_output_accuracy: 0.9310 - pos_output_accuracy: 0.9086 - val_loss: 0.5470 - val_ner_output_accuracy: 0.9300 - val_pos_output_accuracy: 0.9075\n","Epoch 6/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 238ms/step - loss: 0.5295 - ner_output_accuracy: 0.9317 - pos_output_accuracy: 0.9100 - val_loss: 0.5394 - val_ner_output_accuracy: 0.9312 - val_pos_output_accuracy: 0.9083\n","Epoch 7/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 204ms/step - loss: 0.5196 - ner_output_accuracy: 0.9324 - pos_output_accuracy: 0.9105 - val_loss: 0.5325 - val_ner_output_accuracy: 0.9315 - val_pos_output_accuracy: 0.9088\n","Epoch 8/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 236ms/step - loss: 0.5156 - ner_output_accuracy: 0.9336 - pos_output_accuracy: 0.9108 - val_loss: 0.5271 - val_ner_output_accuracy: 0.9327 - val_pos_output_accuracy: 0.9094\n","Epoch 9/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 236ms/step - loss: 0.5066 - ner_output_accuracy: 0.9341 - pos_output_accuracy: 0.9122 - val_loss: 0.5231 - val_ner_output_accuracy: 0.9337 - val_pos_output_accuracy: 0.9104\n","Epoch 10/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 237ms/step - loss: 0.5050 - ner_output_accuracy: 0.9346 - pos_output_accuracy: 0.9118 - val_loss: 0.5185 - val_ner_output_accuracy: 0.9337 - val_pos_output_accuracy: 0.9107\n","Epoch 11/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 235ms/step - loss: 0.4956 - ner_output_accuracy: 0.9361 - pos_output_accuracy: 0.9131 - val_loss: 0.5155 - val_ner_output_accuracy: 0.9344 - val_pos_output_accuracy: 0.9107\n","Epoch 12/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 205ms/step - loss: 0.4914 - ner_output_accuracy: 0.9363 - pos_output_accuracy: 0.9131 - val_loss: 0.5137 - val_ner_output_accuracy: 0.9351 - val_pos_output_accuracy: 0.9111\n","Epoch 13/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 199ms/step - loss: 0.4915 - ner_output_accuracy: 0.9366 - pos_output_accuracy: 0.9134 - val_loss: 0.5094 - val_ner_output_accuracy: 0.9354 - val_pos_output_accuracy: 0.9107\n","Epoch 14/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - loss: 0.4873 - ner_output_accuracy: 0.9365 - pos_output_accuracy: 0.9140 - val_loss: 0.5071 - val_ner_output_accuracy: 0.9352 - val_pos_output_accuracy: 0.9112\n","Epoch 15/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 198ms/step - loss: 0.4782 - ner_output_accuracy: 0.9383 - pos_output_accuracy: 0.9147 - val_loss: 0.5045 - val_ner_output_accuracy: 0.9360 - val_pos_output_accuracy: 0.9120\n","Epoch 16/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - loss: 0.4818 - ner_output_accuracy: 0.9374 - pos_output_accuracy: 0.9138 - val_loss: 0.5029 - val_ner_output_accuracy: 0.9364 - val_pos_output_accuracy: 0.9118\n","Epoch 17/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 227ms/step - loss: 0.4803 - ner_output_accuracy: 0.9376 - pos_output_accuracy: 0.9142 - val_loss: 0.5001 - val_ner_output_accuracy: 0.9366 - val_pos_output_accuracy: 0.9120\n","Epoch 18/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 227ms/step - loss: 0.4773 - ner_output_accuracy: 0.9380 - pos_output_accuracy: 0.9142 - val_loss: 0.4992 - val_ner_output_accuracy: 0.9370 - val_pos_output_accuracy: 0.9122\n","Epoch 19/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 228ms/step - loss: 0.4692 - ner_output_accuracy: 0.9393 - pos_output_accuracy: 0.9158 - val_loss: 0.4963 - val_ner_output_accuracy: 0.9369 - val_pos_output_accuracy: 0.9124\n","Epoch 20/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 228ms/step - loss: 0.4681 - ner_output_accuracy: 0.9389 - pos_output_accuracy: 0.9155 - val_loss: 0.4951 - val_ner_output_accuracy: 0.9372 - val_pos_output_accuracy: 0.9129\n","Epoch 21/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 197ms/step - loss: 0.4672 - ner_output_accuracy: 0.9396 - pos_output_accuracy: 0.9154 - val_loss: 0.4941 - val_ner_output_accuracy: 0.9368 - val_pos_output_accuracy: 0.9127\n","Epoch 22/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - loss: 0.4674 - ner_output_accuracy: 0.9389 - pos_output_accuracy: 0.9159 - val_loss: 0.4929 - val_ner_output_accuracy: 0.9374 - val_pos_output_accuracy: 0.9136\n","Epoch 23/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 195ms/step - loss: 0.4633 - ner_output_accuracy: 0.9405 - pos_output_accuracy: 0.9156 - val_loss: 0.4936 - val_ner_output_accuracy: 0.9375 - val_pos_output_accuracy: 0.9136\n","Epoch 24/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 232ms/step - loss: 0.4557 - ner_output_accuracy: 0.9406 - pos_output_accuracy: 0.9169 - val_loss: 0.4889 - val_ner_output_accuracy: 0.9380 - val_pos_output_accuracy: 0.9134\n","Epoch 25/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 239ms/step - loss: 0.4551 - ner_output_accuracy: 0.9408 - pos_output_accuracy: 0.9171 - val_loss: 0.4883 - val_ner_output_accuracy: 0.9380 - val_pos_output_accuracy: 0.9137\n","Epoch 26/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 234ms/step - loss: 0.4569 - ner_output_accuracy: 0.9408 - pos_output_accuracy: 0.9163 - val_loss: 0.4866 - val_ner_output_accuracy: 0.9380 - val_pos_output_accuracy: 0.9135\n","Epoch 27/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 208ms/step - loss: 0.4514 - ner_output_accuracy: 0.9412 - pos_output_accuracy: 0.9176 - val_loss: 0.4857 - val_ner_output_accuracy: 0.9379 - val_pos_output_accuracy: 0.9134\n","Epoch 28/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 205ms/step - loss: 0.4490 - ner_output_accuracy: 0.9422 - pos_output_accuracy: 0.9174 - val_loss: 0.4841 - val_ner_output_accuracy: 0.9385 - val_pos_output_accuracy: 0.9136\n","Epoch 29/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 236ms/step - loss: 0.4501 - ner_output_accuracy: 0.9411 - pos_output_accuracy: 0.9172 - val_loss: 0.4871 - val_ner_output_accuracy: 0.9380 - val_pos_output_accuracy: 0.9142\n","Epoch 30/30\n","\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 236ms/step - loss: 0.4478 - ner_output_accuracy: 0.9415 - pos_output_accuracy: 0.9180 - val_loss: 0.4821 - val_ner_output_accuracy: 0.9390 - val_pos_output_accuracy: 0.9144\n","Restoring model weights from the end of the best epoch: 30.\n"]}]},{"cell_type":"code","source":["# If you have test data, prepare and evaluate similarly\n","# Example test data preparation\n","X_test_padded = pad_sequences(X_test, maxlen=max_seq_length)\n","y_test_ner_padded = pad_sequences(y_test_ner, maxlen=max_seq_length)\n","y_test_pos_padded = pad_sequences(y_test_pos, maxlen=max_seq_length)\n","\n","X_test_tensor = tf.convert_to_tensor(X_test_padded)\n","y_test_ner_tensor = tf.convert_to_tensor(y_test_ner_padded)\n","y_test_pos_tensor = tf.convert_to_tensor(y_test_pos_padded)\n","\n","attention_mask_test = np.ones_like(X_test_tensor)\n","\n","# Evaluate the model\n","evaluation = model.evaluate(\n","    {'input_ids': X_test_tensor, 'attention_mask': attention_mask_test},\n","    {'ner_output': y_test_ner_tensor, 'pos_output': y_test_pos_tensor}\n",")\n","\n","print(f\"Evaluation results: {evaluation}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PoXMwZrXrXY","outputId":"a1d9923e-68ab-4b4c-8389-d28fc99afa57","executionInfo":{"status":"ok","timestamp":1723564302768,"user_tz":-360,"elapsed":24184,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 356ms/step - loss: 0.4737 - ner_output_accuracy: 0.9410 - pos_output_accuracy: 0.9155\n","Evaluation results: [0.47288796305656433, 0.9408223628997803, 0.9157014489173889]\n"]}]},{"cell_type":"code","source":["\n","# Make predictions\n","predictions = model.predict({'input_ids': X_test_tensor, 'attention_mask': attention_mask_test})\n","\n","# Get the predicted labels\n","pred_ner = np.argmax(predictions['ner_output'], axis=-1)\n","pred_pos = np.argmax(predictions['pos_output'], axis=-1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3gsvjR2OKWK","executionInfo":{"status":"ok","timestamp":1723564326301,"user_tz":-360,"elapsed":18317,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"d7f6cf7d-803e-4eac-ff9c-2b4e5e2c88dc"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 387ms/step\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","# Flatten the true and predicted labels\n","y_true_ner = np.concatenate([y_test_ner.numpy().flatten()])\n","y_pred_ner = np.concatenate([pred_ner.flatten()])\n","\n","# Compute confusion matrix\n","confusion_mat_ner = confusion_matrix(y_true_ner, y_pred_ner)\n","print(\"Confusion Matrix (NER):\\n\", confusion_mat_ner)\n","\n","# Flatten the true and predicted labels\n","y_true_pos = np.concatenate([y_test_pos.numpy().flatten()])\n","y_pred_pos = np.concatenate([pred_pos.flatten()])\n","\n","# Compute confusion matrix\n","confusion_mat_pos = confusion_matrix(y_true_pos, y_pred_pos)\n","print(\"Confusion Matrix (POS):\\n\", confusion_mat_pos)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5nrdImLPaBu","executionInfo":{"status":"ok","timestamp":1723564339781,"user_tz":-360,"elapsed":522,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"0f3101b9-731e-4510-8703-20f77611d13d"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix (NER):\n"," [[84028     0     0     0     0     0     0  1107     1     0     0     7\n","      0     0     0     0     0     1     6     0     0]\n"," [   18     0     0     0     0     0     1   123     1     0     0     0\n","      0     0     0     0     0     1     2     0     0]\n"," [   30     0     2     0     0     0     1   234     3     0     0     2\n","      0     0     0     0     0     1     2     0     0]\n"," [    7     0     0     0     0     0     0   111     0     0     0     0\n","      0     0     0     0     0     1     2     0     0]\n"," [    4     0     0     0     0     0     0    57     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [   43     0     0     0     0     0     1   303     0     0     0     3\n","      0     0     0     0     0     0     0     0     0]\n"," [   59     0     0     0     0     0     1   330     3     0     0     1\n","      0     0     0     0     0     2     4     0     0]\n"," [ 1267     0     2     0     0     4     3  6814    23     0     0    21\n","      0     0     0     0     0     3    12     0     0]\n"," [  114     0     0     0     0     0     0   743    21     0     0     0\n","      0     0     0     0     0     2    15     0     0]\n"," [    1     0     0     0     0     0     0    14     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [   14     0     0     0     0     0     0    67     1     0     2     1\n","      0     0     0     0     0     0     0     0     0]\n"," [   13     0     0     0     0     0     0   118     0     0     0    61\n","      0     0     0     0     0     0     0     0     0]\n"," [    6     0     0     0     0     0     0    61     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [    2     0     0     0     0     0     0    21     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [    6     0     0     0     0     0     0    55     0     0     0     0\n","      0     0     0     0     0     1     1     0     0]\n"," [    2     0     0     0     0     0     0     9     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [   16     0     0     0     0     0     0    51     0     0     0     0\n","      0     0     0     0     0     0     2     0     0]\n"," [   31     0     0     0     0     0     2   208     5     0     0     0\n","      0     0     0     0     0    16    11     0     0]\n"," [   72     0     0     0     0     0     0   227     7     0     0     0\n","      0     0     0     0     0     2    25     0     0]\n"," [    1     0     0     0     0     0     0     7     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]\n"," [    2     0     0     0     0     0     0     4     0     0     0     0\n","      0     0     0     0     0     0     0     0     0]]\n","Confusion Matrix (POS):\n"," [[84933     0     0     0     0  1053   148     0     0     0     2     0\n","      6    14     1]\n"," [  102     1     0     0     0   239    25     0     0     0     1     0\n","      1     8     0]\n"," [   55     0     0     0     0   173    24     0     0     0     0     0\n","      0     1     0]\n"," [   49     0     0     0     0   141    16     0     0     0     0     0\n","      0     1     0]\n"," [    1     0     0     0     0     5     0     0     0     0     0     0\n","      0     0     0]\n"," [ 1294     1     1     0     0  2906   377     0     0     0     4     3\n","     12    20     3]\n"," [  410     0     0     0     0   868   621     0     0     0     1     0\n","      7     4     1]\n"," [    1     0     0     0     0    17     4     0     0     0     0     0\n","      0     0     0]\n"," [    3     0     0     0     0    15     2     0     0     0     0     0\n","      0     0     0]\n"," [  136     1     0     0     0   321    18     0     0     5     0     0\n","      0     3     0]\n"," [   78     0     0     0     0   241    18     0     0     0    12     0\n","      0     3     3]\n"," [   41     1     0     0     0   132    56     0     0     0     0     6\n","      0     3     0]\n"," [  113     0     0     0     0   297    58     0     0     0     0     0\n","     24     2     0]\n"," [  592     0     0     0     0   459    54     0     0     0     5     0\n","      3    30     0]\n"," [  146     0     0     0     0   240     8     0     0     0     0     0\n","      0     5     3]]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Compute classification report\n","report_ner = classification_report(y_true_ner, y_pred_ner)\n","print(\"Classification Report (NER):\\n\", report_ner)\n","\n","\n","# Compute classification report\n","report_pos = classification_report(y_true_pos, y_pred_pos)\n","print(\"Classification Report (POS):\\n\", report_pos)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F3Rx_3dPloJ","executionInfo":{"status":"ok","timestamp":1723564351677,"user_tz":-360,"elapsed":505,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"55658fa6-b227-4f98-e30f-46313283c32c"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Classification Report (NER):\n","               precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.98     85150\n","           1       0.00      0.00      0.00       146\n","           2       0.50      0.01      0.01       275\n","           3       0.00      0.00      0.00       121\n","           4       0.00      0.00      0.00        61\n","           5       0.00      0.00      0.00       350\n","           6       0.11      0.00      0.00       400\n","           7       0.64      0.84      0.72      8149\n","           8       0.32      0.02      0.04       895\n","           9       0.00      0.00      0.00        15\n","          10       1.00      0.02      0.05        85\n","          11       0.64      0.32      0.42       192\n","          12       0.00      0.00      0.00        67\n","          13       0.00      0.00      0.00        23\n","          14       0.00      0.00      0.00        63\n","          15       0.00      0.00      0.00        11\n","          16       0.00      0.00      0.00        69\n","          17       0.53      0.06      0.11       273\n","          18       0.30      0.08      0.12       333\n","          19       0.00      0.00      0.00         8\n","          20       0.00      0.00      0.00         6\n","\n","    accuracy                           0.94     96692\n","   macro avg       0.24      0.11      0.12     96692\n","weighted avg       0.93      0.94      0.93     96692\n","\n","Classification Report (POS):\n","               precision    recall  f1-score   support\n","\n","           0       0.97      0.99      0.98     86157\n","           1       0.25      0.00      0.01       377\n","           2       0.00      0.00      0.00       253\n","           3       0.00      0.00      0.00       207\n","           4       0.00      0.00      0.00         6\n","           5       0.41      0.63      0.50      4621\n","           6       0.43      0.32      0.37      1912\n","           7       0.00      0.00      0.00        22\n","           8       0.00      0.00      0.00        20\n","           9       1.00      0.01      0.02       484\n","          10       0.48      0.03      0.06       355\n","          11       0.67      0.03      0.05       239\n","          12       0.45      0.05      0.09       494\n","          13       0.32      0.03      0.05      1143\n","          14       0.27      0.01      0.01       402\n","\n","    accuracy                           0.92     96692\n","   macro avg       0.35      0.14      0.14     96692\n","weighted avg       0.91      0.92      0.90     96692\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n","\n","# For NER\n","f1_ner = f1_score(y_true_ner, y_pred_ner, average='weighted', zero_division=0)\n","accuracy_ner = accuracy_score(y_true_ner, y_pred_ner)\n","recall_ner = recall_score(y_true_ner, y_pred_ner, average='weighted', zero_division=0)\n","precision_ner = precision_score(y_true_ner, y_pred_ner, average='weighted', zero_division=0)\n","\n","print(\"NER Metrics:\")\n","print(\"F1 Score:\", f1_ner)\n","print(\"Accuracy:\", accuracy_ner)\n","print(\"Recall:\", recall_ner)\n","print(\"Precision:\", precision_ner)\n","\n","# For POS\n","f1_pos = f1_score(y_true_pos, y_pred_pos, average='weighted', zero_division=0)\n","accuracy_pos = accuracy_score(y_true_pos, y_pred_pos)\n","recall_pos = recall_score(y_true_pos, y_pred_pos, average='weighted', zero_division=0)\n","precision_pos = precision_score(y_true_pos, y_pred_pos, average='weighted', zero_division=0)\n","\n","print(\"POS Metrics:\")\n","print(\"F1 Score:\", f1_pos)\n","print(\"Accuracy:\", accuracy_pos)\n","print(\"Recall:\", recall_pos)\n","print(\"Precision:\", precision_pos)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWaffaVjNPKA","executionInfo":{"status":"ok","timestamp":1723564357940,"user_tz":-360,"elapsed":497,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"55fdab04-1ac6-4005-807c-0c477e95ba14"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["NER Metrics:\n","F1 Score: 0.9291583265097709\n","Accuracy: 0.9408224051627849\n","Recall: 0.9408224051627849\n","Precision: 0.926507689972078\n","POS Metrics:\n","F1 Score: 0.9019121154940104\n","Accuracy: 0.9157014023910975\n","Recall: 0.9157014023910975\n","Precision: 0.9051857188454433\n"]}]},{"cell_type":"code","source":["# Save the trained model\n","model.save('file_path/saved_model.keras')"],"metadata":{"id":"QLEx1EnFdagi","executionInfo":{"status":"ok","timestamp":1723564370053,"user_tz":-360,"elapsed":483,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["hidden_dim = 256\n","ner_classes = 21  # Number of NER classes\n","pos_classes = 15\n","loaded_model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","loaded_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","    metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'}\n",")\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/GTL_dataset/saved_model.keras', custom_objects={'BertMultiTaskModel': BertMultiTaskModel})\n","loaded_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":584},"id":"p_oap-69MC0h","executionInfo":{"status":"error","timestamp":1723564760820,"user_tz":-360,"elapsed":7740,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"466acf7b-8e84-4422-e536-1659ed02dbbf"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"TypeError","evalue":"BertMultiTaskModel.__init__() missing 3 required positional arguments: 'hidden_dim', 'ner_classes', and 'pos_classes'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-3d4f8c3511c8>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ner_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GTL_dataset/saved_model.keras'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'BertMultiTaskModel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBertMultiTaskModel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mbuild_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"build_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuild_config\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mcompile_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compile_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-74-104a369941f0>\u001b[0m in \u001b[0;36mbuild_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m92\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example input shape; adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: BertMultiTaskModel.__init__() missing 3 required positional arguments: 'hidden_dim', 'ner_classes', and 'pos_classes'"]}]},{"cell_type":"code","source":["from fastapi import FastAPI, HTTPException\n","from pydantic import BaseModel\n","import tensorflow as tf\n","from transformers import BertTokenizer\n","import numpy as np\n","\n","app = FastAPI()\n","\n","# Load the model and tokenizer\n","model_path = \"path/to/your/saved/model\"  # Update with the path to your saved model\n","model = tf.keras.models.load_model(model_path, custom_objects={'BertMultiTaskModel': BertMultiTaskModel})\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Define the input data model\n","class InferenceRequest(BaseModel):\n","    sentences: list[str]\n","\n","# Define health check endpoint\n","@app.get(\"/health\")\n","async def health_check():\n","    return {\"status\": \"healthy\"}\n","\n","# Define prediction endpoint\n","@app.post(\"/predict\")\n","async def predict(request: InferenceRequest):\n","    sentences = request.sentences\n","\n","    # Tokenize the sentences\n","    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=92, return_tensors='tf')\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","\n","    # Perform inference\n","    predictions = model({\n","        'input_ids': input_ids,\n","        'attention_mask': attention_mask\n","    })\n","\n","    # Extract predictions\n","    ner_predictions = np.argmax(predictions['ner_output'], axis=-1).tolist()\n","    pos_predictions = np.argmax(predictions['pos_output'], axis=-1).tolist()\n","\n","    return {\n","        \"ner_predictions\": ner_predictions,\n","        \"pos_predictions\": pos_predictions\n","    }\n","\n","if __name__ == \"__main__\":\n","    import uvicorn\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"z-052kS52PKs","executionInfo":{"status":"error","timestamp":1723564618229,"user_tz":-360,"elapsed":497,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"ebc5c8a5-d248-4ad7-fdc1-8ee45671a401"},"execution_count":86,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'fastapi'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-23d3ac3a168e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTTPException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFBertModel, BertTokenizer\n","\n","class BertMultiTaskModel(tf.keras.Model):\n","    def __init__(self, hidden_dim, ner_classes, pos_classes):\n","        super(BertMultiTaskModel, self).__init__()\n","        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.hidden_dim = hidden_dim\n","        self.ner_classes = ner_classes\n","        self.pos_classes = pos_classes\n","        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n","        self.ner_output = tf.keras.layers.Dense(ner_classes, activation='softmax', name='ner_output')\n","        self.pos_output = tf.keras.layers.Dense(pos_classes, activation='softmax', name='pos_output')\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        x = self.dense(sequence_output)\n","        ner_logits = self.ner_output(x)\n","        pos_logits = self.pos_output(x)\n","        return {'ner_output': ner_logits, 'pos_output': pos_logits}\n","\n","    def get_config(self):\n","        return {\n","            'hidden_dim': self.hidden_dim,\n","            'ner_classes': self.ner_classes,\n","            'pos_classes': self.pos_classes\n","        }\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n","# Initialize and compile the model\n","hidden_dim = 256\n","ner_classes = 21\n","pos_classes = 15\n","model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","    loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","    metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'}\n",")\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/GTL_dataset/mod_save.weights.h5')\n","\n","# To load the model, first reinitialize the architecture\n","loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/GTL_dataset/mod_save.weights.h5', custom_objects={'BertMultiTaskModel': BertMultiTaskModel})\n","loaded_model.summary()\n","# Now you can use `loaded_model` for inference or further training\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"6eHiOz3cFWt5","executionInfo":{"status":"ok","timestamp":1723558507457,"user_tz":-360,"elapsed":6650,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"6d5f30ef-7757-4d98-b0b3-7ddade3b16bb"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"bert_multi_task_model_10\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_multi_task_model_10\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["from transformers import TFBertModel, BertTokenizer\n","import tensorflow as tf\n","\n","# Define the model\n","class BertMultiTaskModel(tf.keras.Model):\n","    def __init__(self, hidden_dim, ner_classes, pos_classes):\n","        super(BertMultiTaskModel, self).__init__()\n","        self.bert = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.hidden_dim = hidden_dim\n","        self.ner_classes = ner_classes\n","        self.pos_classes = pos_classes\n","        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n","        self.ner_output = tf.keras.layers.Dense(ner_classes, activation='softmax', name='ner_output')\n","        self.pos_output = tf.keras.layers.Dense(pos_classes, activation='softmax', name='pos_output')\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        x = self.dense(sequence_output)\n","        ner_logits = self.ner_output(x)\n","        pos_logits = self.pos_output(x)\n","        return {'ner_output': ner_logits, 'pos_output': pos_logits}\n","\n","    def get_config(self):\n","        return {\n","            'hidden_dim': self.hidden_dim,\n","            'ner_classes': self.ner_classes,\n","            'pos_classes': self.pos_classes\n","        }\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n","# Initialize model\n","hidden_dim = 256\n","ner_classes = 10  # Example number of NER classes\n","pos_classes = 20  # Example number of POS classes\n","input_ids = Input(shape=(92,), dtype=tf.int32, name='input_ids')\n","attention_mask = Input(shape=(92,), dtype=tf.int32, name='attention_mask')\n","\n","upld_model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","upld_model({'input_ids': input_ids, 'attention_mask': attention_mask})  # This will build the model\n","\n","# Compile the model\n","upld_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","    loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","    metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'}\n",")\n","\n","# Load weights (if you have pre-trained weights)\n","upld_model.load_weights('/content/drive/MyDrive/GTL_dataset/model_save.weights.h5')\n","upld_model.summary()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":619},"id":"x7DNfQMvDdZB","executionInfo":{"status":"error","timestamp":1723558065147,"user_tz":-360,"elapsed":8752,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"8b00a652-ca32-47cc-c420-cfb26c9343ed"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"A total of 2 objects could not be loaded. Example error message for object <Dense name=ner_output, built=True>:\n\nLayer 'ner_output' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=ner_output, built=True>, <Dense name=pos_output, built=True>]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-bced6ae7eb09>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Load weights (if you have pre-trained weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mupld_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GTL_dataset/model_save.weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mupld_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A total of 2 objects could not be loaded. Example error message for object <Dense name=ner_output, built=True>:\n\nLayer 'ner_output' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=ner_output, built=True>, <Dense name=pos_output, built=True>]"]}]},{"cell_type":"code","source":["\n","hidden_dim = 256\n","ner_classes = 21\n","pos_classes = 15\n","\n","upld_model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","#input_shape = (92,)  # Assuming your input shape is (batch_size, 92)\n","#model.build(input_shape=(None, input_shape[0]))\n","\n","# Compile the model (if needed)\n","\n","upld_model.load_weights('/content/drive/MyDrive/GTL_dataset/model_save.weights.h5')\n","upld_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"id":"lXwETz-C18oj","executionInfo":{"status":"ok","timestamp":1723557718690,"user_tz":-360,"elapsed":2409,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"1e3f61f7-761e-45fd-fa05-92bb73e168c7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"bert_multi_task_model_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_multi_task_model_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"Mjb1xsFv3Mmh","executionInfo":{"status":"ok","timestamp":1723554968976,"user_tz":-360,"elapsed":549,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"1a19a5fd-e8b5-4674-c3de-b742935084ee"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"bert_multi_task_model_6\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_multi_task_model_6\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ ner_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ pos_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Example sentence for inference\n","test_sentence = [\"আমি স্কুলে যাচ্ছি।\"]  # Replace with your sentence\n","\n","# Tokenize and preprocess the sentence\n","inputs = tokenizer(test_sentence, padding=True, truncation=True, max_length=50, return_tensors='tf')\n","input_ids = inputs['input_ids']\n","attention_mask = inputs['attention_mask']\n","input_ids = inputs(shape=(92,), dtype=tf.int32, name='input_ids')\n","attention_mask = inputs(shape=(92,), dtype=tf.int32, name='attention_mask')\n","\n","inputs = {\n","    'input_ids': input_ids,             # Replace with actual input_ids tensor\n","    'attention_mask': attention_mask    # Replace with actual attention_mask tensor\n","}\n","\n","# Call the model with the inputs dictionary\n","outputs = model(inputs)\n","outputs\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"8edMVbDC8mgk","executionInfo":{"status":"error","timestamp":1723556641315,"user_tz":-360,"elapsed":821,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"52bc807d-d498-46d1-f9a0-51208e96cb64"},"execution_count":36,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'BatchEncoding' object is not callable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-ae63c03ed6ef>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m92\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m92\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'BatchEncoding' object is not callable"]}]},{"cell_type":"code","source":["# Perform inference\n","ner_prediction, pos_prediction = upld_model.predict([input_ids, attention_mask])\n","\n","# Convert predictions to labels (assuming softmax output)\n","ner_labels = tf.argmax(ner_prediction, axis=-1)\n","pos_labels = tf.argmax(pos_prediction, axis=-1)\n","\n","print(\"NER Prediction:\", ner_labels)\n","print(\"POS Prediction:\", pos_labels)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"UsZ0Eoes8vhx","executionInfo":{"status":"error","timestamp":1723556366698,"user_tz":-360,"elapsed":1416,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"dde3294c-43e7-43e6-f005-218697c74a3c"},"execution_count":34,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Exception encountered when calling BertMultiTaskModel.call().\n\n\u001b[1mtuple indices must be integers or slices, not str\u001b[0m\n\nArguments received by BertMultiTaskModel.call():\n  • inputs=('tf.Tensor(shape=(1, 12), dtype=int32)', 'tf.Tensor(shape=(1, 12), dtype=int32)')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-6410e5328a46>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mner_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupld_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert predictions to labels (assuming softmax output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mner_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-09b40c6b744a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling BertMultiTaskModel.call().\n\n\u001b[1mtuple indices must be integers or slices, not str\u001b[0m\n\nArguments received by BertMultiTaskModel.call():\n  • inputs=('tf.Tensor(shape=(1, 12), dtype=int32)', 'tf.Tensor(shape=(1, 12), dtype=int32)')"]}]},{"cell_type":"code","source":["model = tf.keras.models.load_model('/content/drive/MyDrive/GTL_dataset/simple_model.h5', custom_objects={'BertMultiTaskModel': BertMultiTaskModel})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"96vFZWFE1x0e","executionInfo":{"status":"error","timestamp":1723554303005,"user_tz":-360,"elapsed":2497,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}},"outputId":"b47dc2a6-2384-40a0-9b46-c941687cc063"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Layer count mismatch when loading weights from file. Model expected 0 layers, found 3 saved layers.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-ea4d27270016>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GTL_dataset/simple_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'BertMultiTaskModel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBertMultiTaskModel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;34m\"Layer count mismatch when loading weights from file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;34mf\"Model expected {len(filtered_layers)} layers, found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 0 layers, found 3 saved layers."]}]},{"cell_type":"code","source":["from tensorflow.keras.utils import CustomObjectScope\n","from tensorflow.keras.models import load_model\n","\n","model = BertMultiTaskModel(hidden_dim, ner_classes, pos_classes)\n","\n","# Register the custom model class\n","with CustomObjectScope({'BertMultiTaskModel': BertMultiTaskModel}):\n","    loaded_model = load_model('/content/drive/MyDrive/GTL_dataset/simple_model.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"collapsed":true,"id":"9Jd48sbzdnYh","outputId":"fe764e48-522e-429f-93e2-0b14275694c5","executionInfo":{"status":"error","timestamp":1723553869232,"user_tz":-360,"elapsed":3322,"user":{"displayName":"Sharif Jaman","userId":"06492460293547197459"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Layer count mismatch when loading weights from file. Model expected 0 layers, found 3 saved layers.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e053f2d8f1d6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Register the custom model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'BertMultiTaskModel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBertMultiTaskModel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GTL_dataset/simple_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;34m\"Layer count mismatch when loading weights from file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;34mf\"Model expected {len(filtered_layers)} layers, found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 0 layers, found 3 saved layers."]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import custom_object_scope\n","from tensorflow.keras.models import load_model\n","\n","class SimpleMultiTaskModel(Model):\n","    def __init__(self, hidden_dim, ner_classes, pos_classes, **kwargs):\n","        super(SimpleMultiTaskModel, self).__init__(**kwargs)\n","        self.hidden_dim = hidden_dim\n","        self.ner_classes = ner_classes\n","        self.pos_classes = pos_classes\n","        # Define layers\n","        self.dense_ner = Dense(ner_classes, activation='softmax', name='ner_output')\n","        self.dense_pos = Dense(pos_classes, activation='softmax', name='pos_output')\n","\n","    def call(self, inputs):\n","        x = inputs\n","        ner_output = self.dense_ner(x)\n","        pos_output = self.dense_pos(x)\n","        return {'ner_output': ner_output, 'pos_output': pos_output}\n","\n","    def get_config(self):\n","        config = super(SimpleMultiTaskModel, self).get_config()\n","        config.update({\n","            'hidden_dim': self.hidden_dim,\n","            'ner_classes': self.ner_classes,\n","            'pos_classes': self.pos_classes,\n","        })\n","        return config\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n","# Define input shape and create a model instance\n","input_shape = (128,)  # Adjust based on your input shape\n","inputs = Input(shape=input_shape)\n","model = SimpleMultiTaskModel(hidden_dim=256, ner_classes=10, pos_classes=20)\n","outputs = model(inputs)\n","model = Model(inputs, outputs)\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","              metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'})\n","\n","# Build the model explicitly by passing a dummy input\n","model.build((None, *input_shape))\n","\n","# Save the model\n","model.save('/content/drive/MyDrive/GTL_dataset/simple_model.h5')\n","\n","# Load the model with custom objects\n","with custom_object_scope({'SimpleMultiTaskModel': SimpleMultiTaskModel}):\n","    loaded_model = load_model('/content/drive/MyDrive/GTL_dataset/simple_model.h5')\n","\n","# Verify the model is loaded correctly\n","loaded_model.summary()\n"],"metadata":{"id":"U4Yt7HfpjVVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-Ftxnv5Dj2Er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example sentence for inference\n","test_sentence = [\"আমি স্কুলে যাচ্ছি।\"]  # Replace with your sentence\n","\n","# Tokenize and preprocess the sentence\n","inputs = tokenizer(test_sentence, padding=True, truncation=True, max_length=50, return_tensors='tf')\n","input_ids = inputs['input_ids']\n","attention_mask = inputs['attention_mask']\n"],"metadata":{"id":"ESlcwHJJdoez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform inference\n","ner_prediction, pos_prediction = loaded_model.predict([input_ids, attention_mask])\n","\n","# Convert predictions to labels (assuming softmax output)\n","ner_labels = tf.argmax(ner_prediction, axis=-1)\n","pos_labels = tf.argmax(pos_prediction, axis=-1)\n","\n","print(\"NER Prediction:\", ner_labels)\n","print(\"POS Prediction:\", pos_labels)\n"],"metadata":{"id":"sQGVBjwXdvGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example mapping dictionaries (assuming you have these)\n","ner_tags = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n","pos_label_map = {0: 'NOUN', 1: 'VERB', 2: 'ADJ', 3: 'ADV', 4: 'PRON', 5: 'DET', 6: 'ADP'}\n","\n","# Convert predictions to human-readable labels\n","ner_prediction_labels = [ner_label_map[label] for label in ner_labels.numpy()[0]]\n","pos_prediction_labels = [pos_label_map[label] for label in pos_labels.numpy()[0]]\n","\n","print(\"NER Prediction Labels:\", ner_prediction_labels)\n","print(\"POS Prediction Labels:\", pos_prediction_labels)\n"],"metadata":{"id":"QPh0YFbWd3Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers tensorflow tensorflow-addons\n"],"metadata":{"id":"l9SPA04kSEQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFBertModel, BertTokenizer\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers, Model\n","\n","# Load pre-trained BERT model and tokenizer\n","bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Input layers\n","input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n","attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n","\n","# BERT model\n","bert_output = bert_model(input_ids, attention_mask=attention_mask)\n","sequence_output = bert_output.last_hidden_state\n","\n","# NER and POS CRF layers\n","ner_crf = tfa.layers.CRF(num_classes=ner_classes, name=\"ner_crf\")\n","pos_crf = tfa.layers.CRF(num_classes=pos_classes, name=\"pos_crf\")\n","\n","# CRF outputs\n","ner_output = ner_crf(sequence_output)\n","pos_output = pos_crf(sequence_output)\n","\n","# Model definition\n","model = Model(inputs=[input_ids, attention_mask], outputs=[ner_output, pos_output])\n","\n","# Compile model with loss and optimizer\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n","              loss={'ner_crf': ner_crf.loss, 'pos_crf': pos_crf.loss},\n","              metrics={'ner_crf': 'accuracy', 'pos_crf': 'accuracy'})\n","\n","model.summary()\n"],"metadata":{"id":"CD-V_YRVVbCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example sentences and tags\n","#sentences = [\"আমি স্কুলে যাই।\", \"সে একটি বড় বই পড়ছে।\"]\n","#ner_tags = [[0, 1, 2, 3, 0], [0, 1, 2, 3, 4, 0]]\n","#pos_tags = [[0, 1, 2, 3, 0], [0, 1, 2, 3, 4, 0]]\n","\n","# Tokenize sentences\n","inputs = tokenizer(sentences, padding=True, truncation=True, max_length=50, return_tensors='tf')\n","\n","input_ids = inputs['input_ids']\n","attention_mask = inputs['attention_mask']\n","\n","# Convert tags to tensors and pad\n","ner_tags = tf.keras.preprocessing.sequence.pad_sequences(ner_tags, maxlen=50, padding='post')\n","pos_tags = tf.keras.preprocessing.sequence.pad_sequences(pos_tags, maxlen=50, padding='post')\n","\n","# Convert to tensors\n","ner_tags = tf.convert_to_tensor(ner_tags)\n","pos_tags = tf.convert_to_tensor(pos_tags)\n"],"metadata":{"id":"eGbcM6VMVe4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    x={'input_ids': input_ids, 'attention_mask': attention_mask},\n","    y={'ner_crf': ner_tags, 'pos_crf': pos_tags},\n","    batch_size=32,\n","    epochs=3,\n","    validation_split=0.2\n",")"],"metadata":{"id":"OYAwAiGsV4bw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate model on validation set\n","val_loss, val_ner_acc, val_pos_acc = model.evaluate(\n","    x={'input_ids': val_input_ids, 'attention_mask': val_attention_mask},\n","    y={'ner_crf': val_ner_tags, 'pos_crf': val_pos_tags}\n",")\n","print(f\"Validation NER Accuracy: {val_ner_acc}\")\n","print(f\"Validation POS Accuracy: {val_pos_acc}\")\n"],"metadata":{"id":"iZPzN_5eWUkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow-addons==0.17.0\n","!pip install tensorflow==2.10.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"MGmH88ZCQYo0","outputId":"48a28e7b-7058-4147-908f-e76be109e810"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons==0.17.0 in /usr/local/lib/python3.10/dist-packages (0.17.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.0) (24.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.0) (4.3.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.17.0) (4.12.2)\n","Collecting tensorflow==2.10.0\n","  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.0)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.11.0)\n","Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.1)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n","  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.1)\n","Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n","Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigtable 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-pubsub 2.23.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","google","keras","tensorflow"]},"id":"61bec3a70f3947deb163521481983f3e"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install transformers scikit-learn\n"],"metadata":{"id":"qIGVGYq-Qyu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the multi-task model\n","class MultiTaskModel(tf.keras.Model):\n","    def __init__(self, ner_classes, pos_classes):\n","        super(MultiTaskModel, self).__init__()\n","        self.bert = bert_model\n","        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n","        self.ner_crf = tfa.layers.CRF(ner_classes)\n","        self.pos_crf = tfa.layers.CRF(pos_classes)\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        features = self.dense(sequence_output)\n","\n","        ner_output, _ = self.ner_crf(features)\n","        pos_output, _ = self.pos_crf(features)\n","\n","        return {'ner_output': ner_output, 'pos_output': pos_output}\n","\n","    def get_loss(self, ner_labels, pos_labels, ner_pred, pos_pred):\n","        # Calculate the CRF loss using the built-in CRF loss method\n","        ner_loss = -self.ner_crf.log_likelihood(ner_labels, ner_pred, sequence_lengths=tf.reduce_sum(ner_labels != 0, axis=1))\n","        pos_loss = -self.pos_crf.log_likelihood(pos_labels, pos_pred, sequence_lengths=tf.reduce_sum(pos_labels != 0, axis=1))\n","        return tf.reduce_mean(ner_loss + pos_loss)\n","\n","# Instantiate the model\n","model = MultiTaskModel(ner_classes, pos_classes)\n","\n","# Define a custom loss function that uses model's get_loss method\n","def custom_loss(y_true, y_pred):\n","    ner_labels = y_true['ner_output']\n","    pos_labels = y_true['pos_output']\n","    ner_pred = y_pred['ner_output']\n","    pos_pred = y_pred['pos_output']\n","    return model.get_loss(ner_labels, pos_labels, ner_pred, pos_pred)\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss=custom_loss,\n","              metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'})"],"metadata":{"id":"2Ab8PxbBUuiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming X_train, X_val, X_test, y_train_ner, y_val_ner, y_test_ner, y_train_pos, y_val_pos, y_test_pos are already defined\n","\n","# Train the model\n","history = model.fit(\n","    {'input_ids': X_train, 'attention_mask': attention_mask_train},\n","    {'ner_crf': y_train_ner, 'pos_crf': y_train_pos},\n","    validation_data=(\n","        {'input_ids': X_val, 'attention_mask': attention_mask_val},\n","        {'ner_crf': y_val_ner, 'pos_crf': y_val_pos}\n","    ),\n","    epochs=5,\n","    batch_size=32\n",")\n","\n","# Evaluate the model\n","evaluation = model.evaluate(\n","    {'input_ids': X_test, 'attention_mask': attention_mask_test},\n","    {'ner_crf': y_test_ner, 'pos_crf': y_test_pos}\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"n3rmGRJRSJhb","outputId":"1ad335ae-1b03-45cc-9453-7f8519f711cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filee5hi8qsu.py\", line 15, in tf__call\n        (ner_output, _) = ag__.converted_call(ag__.ld(self).ner_crf, (ag__.ld(features),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"multi_task_model_4\" \"                 f\"(type MultiTaskModel).\n    \n    in user code:\n    \n        File \"<ipython-input-16-0604c038637a>\", line 18, in call  *\n            ner_output, _ = self.ner_crf(features)\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"multi_task_model_4\" \"                 f\"(type MultiTaskModel):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 92), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 92), dtype=int64)'}\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-d62c66746caf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattention_mask_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'ner_crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_pos\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_filee5hi8qsu.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mner_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_crf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mpos_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_crf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filee5hi8qsu.py\", line 15, in tf__call\n        (ner_output, _) = ag__.converted_call(ag__.ld(self).ner_crf, (ag__.ld(features),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"multi_task_model_4\" \"                 f\"(type MultiTaskModel).\n    \n    in user code:\n    \n        File \"<ipython-input-16-0604c038637a>\", line 18, in call  *\n            ner_output, _ = self.ner_crf(features)\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"multi_task_model_4\" \"                 f\"(type MultiTaskModel):\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, 92), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 92), dtype=int64)'}\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFBertModel\n","\n","# Hyperparameters\n","hidden_dim = 256\n","ner_classes = 10  # Number of NER classes\n","pos_classes = 20  # Number of POS classes\n","learning_rate = 0.001\n","\n","# Load pre-trained BERT model\n","bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n","\n","# Define a simpler multi-task model\n","class SimpleMultiTaskModel(tf.keras.Model):\n","    def __init__(self, ner_classes, pos_classes):\n","        super(SimpleMultiTaskModel, self).__init__()\n","        self.bert = bert_model\n","        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n","        self.ner_output_layer = tf.keras.layers.Dense(ner_classes, activation='softmax')\n","        self.pos_output_layer = tf.keras.layers.Dense(pos_classes, activation='softmax')\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        features = self.dense(sequence_output)\n","\n","        ner_output = self.ner_output_layer(features)\n","        pos_output = self.pos_output_layer(features)\n","\n","        return {'ner_output': ner_output, 'pos_output': pos_output}\n","\n","# Instantiate the simpler model\n","simple_model = SimpleMultiTaskModel(ner_classes, pos_classes)\n","\n","# Compile the model with simple categorical crossentropy loss\n","simple_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss={'ner_output': 'sparse_categorical_crossentropy', 'pos_output': 'sparse_categorical_crossentropy'},\n","              metrics={'ner_output': 'accuracy', 'pos_output': 'accuracy'})\n","\n","# Train the simpler model\n","history = simple_model.fit(\n","    {'input_ids': X_train, 'attention_mask': attention_mask_train},\n","    {'ner_output': y_train_ner, 'pos_output': y_train_pos},\n","    validation_data=(\n","        {'input_ids': X_val, 'attention_mask': attention_mask_val},\n","        {'ner_output': y_val_ner, 'pos_output': y_val_pos}\n","    ),\n","    epochs=5,\n","    batch_size=32\n",")\n","\n","# Evaluate the simpler model\n","evaluation = simple_model.evaluate(\n","    {'input_ids': X_test, 'attention_mask': attention_mask_test},\n","    {'ner_output': y_test_ner, 'pos_output': y_test_pos}\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YPBU85M7V5lb","outputId":"9e651dba-a7bf-40e6-a3e7-0a10b37bed57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/metrics_utils.py\", line 970, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 128 and 92 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_3, Cast_4)' with input shapes: [?,128], [?,92].\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-2430fc2b9d10>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Train the simpler model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m history = simple_model.fit(\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattention_mask_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'ner_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_pos\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/metrics/metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/metrics_utils.py\", line 970, in sparse_categorical_matches\n        matches = tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n    ValueError: Dimensions must be equal, but are 128 and 92 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_3, Cast_4)' with input shapes: [?,128], [?,92].\n"]}]},{"cell_type":"code","source":["print(processed_data.shape)\n","print(processed_data.columns)\n","processed_data.info()\n","processed_data.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_GjFr_OFUuu","outputId":"00d73476-9955-4b25-f451-72131d175112","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7002, 3)\n","Index(['Sentence', 'POS Tags', 'NER Tags'], dtype='object')\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7002 entries, 0 to 7001\n","Data columns (total 3 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Sentence  7002 non-null   object\n"," 1   POS Tags  7002 non-null   object\n"," 2   NER Tags  7002 non-null   object\n","dtypes: object(3)\n","memory usage: 164.2+ KB\n"]},{"output_type":"execute_result","data":{"text/plain":["                        Sentence  \\\n","count                       7002   \n","unique                      6991   \n","top     মেষ [২১ মার্চ-২০ এপ্রিল]   \n","freq                           2   \n","\n","                                             POS Tags  \\\n","count                                            7002   \n","unique                                           6809   \n","top     [ADJ, NNC, PUNCT, QF, PART, NNP, QF, NNC, QF]   \n","freq                                               20   \n","\n","                                          NER Tags  \n","count                                         7002  \n","unique                                        4985  \n","top     [B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-OTH]  \n","freq                                           187  "],"text/html":["\n","  <div id=\"df-165c3f28-a814-4701-9f94-788faa9d8c48\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>POS Tags</th>\n","      <th>NER Tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7002</td>\n","      <td>7002</td>\n","      <td>7002</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>6991</td>\n","      <td>6809</td>\n","      <td>4985</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>মেষ [২১ মার্চ-২০ এপ্রিল]</td>\n","      <td>[ADJ, NNC, PUNCT, QF, PART, NNP, QF, NNC, QF]</td>\n","      <td>[B-OTH, B-OTH, B-OTH, B-OTH, B-OTH, B-OTH]</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>2</td>\n","      <td>20</td>\n","      <td>187</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-165c3f28-a814-4701-9f94-788faa9d8c48')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-165c3f28-a814-4701-9f94-788faa9d8c48 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-165c3f28-a814-4701-9f94-788faa9d8c48');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ae81a7f8-332e-4563-b54d-15d0521d9e94\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae81a7f8-332e-4563-b54d-15d0521d9e94')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ae81a7f8-332e-4563-b54d-15d0521d9e94 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"processed_data\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          6991,\n          \"2\",\n          \"7002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS Tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NER Tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["for i in range(3):\n","    print(f\"Sentence: {processed_data['Sentence'][i]}\")\n","    print(f\"POS Tags: {processed_data['POS Tags'][i]}\")\n","    print(f\"NER Tags: {processed_data['NER Tags'][i]}\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pR3A5sDpJk_d","outputId":"113adaf4-eeff-4110-eef3-3e96db56ff5d","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: শনিবার (২৭ আগস্ট) রাতে পটুয়াখালী সদর থানার ভারপ্রাপ্ত কর্মকর্তা (ওসি) মো. মনিরুজ্জামান এ তথ্য নিশ্চিত করেছেন।\n","POS Tags: ['NNP', 'PUNCT', 'NNP', 'NNC', 'NNP', 'NNC', 'NNC', 'ADJ', 'NNC', 'PUNCT', 'NNP', 'NNP', 'DET', 'NNC', 'ADJ', 'VF']\n","NER Tags: ['B-D&T', 'B-OTH', 'B-D&T', 'B-D&T', 'B-GPE', 'I-GPE', 'I-GPE', 'B-PER', 'I-PER', 'B-OTH', 'B-PER', 'I-PER', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH']\n","\n","Sentence: বায়ুদূষণ ও স্মার্ট ফোন ছেলেমেয়ে উভয়ের প্রজনন ক্ষমতা হ্রাস করে দিচ্ছে।\n","POS Tags: ['NNC', 'CONJ', 'NNC', 'NNC', 'NNC', 'PRO', 'NNC', 'NNC', 'NNC', 'VNF', 'VF']\n","NER Tags: ['B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-PER', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH']\n","\n","Sentence: ছাত্র রাজনীতির বর্তমান অবস্থার শুরু হয়েছিলো স্বৈরশাসক এরশাদের হাত ধরে।\n","POS Tags: ['NNC', 'NNC', 'ADJ', 'NNC', 'NNC', 'VF', 'NNC', 'NNP', 'NNC', 'PP']\n","NER Tags: ['B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-OTH', 'B-PER', 'B-PER', 'B-OTH', 'B-OTH']\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zZtizipYLuYc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from transformers import TFBertModel, BertTokenizer\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"collapsed":true,"id":"OhkT_hFHLoWx","outputId":"5c256cd0-29da-466a-c404-edd8b9b7311a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow_addons'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-fc461330e871>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install transformers scikit-learn\n","!pip install tensorflow==2.10.0\n","!pip install tensorflow-addons==0.17.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"E2oguIx8z_lb","outputId":"902d6389-4583-46a5-e3e0-97bfcc6f5971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Collecting tensorflow==2.10.0\n","  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10.0)\n","  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.64.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.11.0)\n","Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10.0)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.1)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10.0)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10.0)\n","  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.1)\n","Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10.0)\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.16.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.3)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n","Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.6.0\n","    Uninstalling gast-0.6.0:\n","      Successfully uninstalled gast-0.6.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.17.0\n","    Uninstalling tensorflow-2.17.0:\n","      Successfully uninstalled tensorflow-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigquery-connection 1.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-bigtable 2.25.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-functions 1.16.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-iam 2.15.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-language 2.13.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-pubsub 2.23.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-resource-manager 1.12.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","google-cloud-translate 3.15.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","google","keras","tensorflow"]},"id":"19825073c4ca423f87199975ad84a4ba"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons==0.17.0\n","  Downloading tensorflow_addons-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.0) (24.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons==0.17.0) (4.3.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=2.7->tensorflow-addons==0.17.0) (4.12.2)\n","Downloading tensorflow_addons-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.17.0\n","^C\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from transformers import TFBertModel, BertTokenizer\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"3_-pDmQfHRKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define hyperparameters\n","#vocab_size = 10000  # Example vocabulary size\n","#embedding_dim = 128\n","#hidden_dim = 256\n","#ner_classes = 10  # Number of NER classes\n","#pos_classes = 20  # Number of POS classes\n","#sequence_length = 50  # Maximum sequence length\n","\n","# Load BERT model and tokenizer\n","bert_model_name = 'bert-base-uncased'\n","bert_model = TFBertModel.from_pretrained(bert_model_name)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","\n","# Define the model\n","class MultiTaskModel(tf.keras.Model):\n","    def __init__(self, ner_classes, pos_classes):\n","        super(MultiTaskModel, self).__init__()\n","        self.bert = bert_model\n","        self.dense = tf.keras.layers.Dense( 256, activation='relu')\n","        self.ner_crf = tfa.layers.CRF(ner_classes)\n","        self.pos_crf = tfa.layers.CRF(pos_classes)\n","\n","    def call(self, inputs, attention_mask):\n","        bert_output = self.bert(inputs, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        features = self.dense(sequence_output)\n","        ner_output = self.ner_crf(features)\n","        pos_output = self.pos_crf(features)\n","        return ner_output, pos_output\n","\n","# Instantiate the model\n","model = MultiTaskModel(ner_classes, pos_classes)\n","model.compile(optimizer=Adam(learning_rate=2e-5),\n","              loss={'crf': lambda y_true, y_pred: ner_crf.compute_loss(y_true, y_pred),\n","                    'crf': lambda y_true, y_pred: pos_crf.compute_loss(y_true, y_pred)},\n","              metrics={'crf': lambda y_true, y_pred: ner_crf.compute_accuracy(y_true, y_pred)})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lIJW-UC_yH1","outputId":"564d1697-4eec-4c5d-aef9-66c1871c42b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_texts = sentences\n","y_ner = ner_tags\n","y_pos = pos_tags\n","\n","# Tokenize and prepare input data\n","inputs = tokenizer(X_texts, padding=True, truncation=True, max_length=50, return_tensors='tf')\n","input_ids = inputs['input_ids']\n","attention_mask = inputs['attention_mask']\n","\n","# Split data into train, validation, and test sets\n","X_train, X_temp, y_train_ner, y_temp_ner = train_test_split(input_ids, y_ner, test_size=0.2, random_state=42)\n","X_val, X_test, y_val_ner, y_test_ner = train_test_split(X_temp, y_temp_ner, test_size=0.2, random_state=42)\n","\n","# Note: For simplicity, assume POS labels are split similarly\n","y_train_pos, y_val_pos, y_test_pos = train_test_split(y_pos, test_size=0.2, random_state=42)\n","y_val_pos, y_test_pos = train_test_split(y_val_pos, test_size=0.2, random_state=42)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"z2TW94RTFqeN","outputId":"5ad7ec3c-8905-4df0-94bf-888adac8faac"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-2f66c5ae1d92>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Tokenize and prepare input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2945\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2946\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3030\u001b[0m                 )\n\u001b[1;32m   3031\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3032\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3033\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3226\u001b[0m         )\n\u001b[1;32m   3227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3228\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3229\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from transformers import TFBertModel, BertTokenizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define hyperparameters\n","vocab_size = 10000  # Example vocabulary size (not directly used here since BERT handles its own vocabulary)\n","embedding_dim = 128  # Dimension of embedding vectors (not used here as BERT embeddings are used)\n","hidden_dim = 256  # Dimension of hidden layer\n","ner_classes = 10  # Number of NER classes\n","pos_classes = 20  # Number of POS classes\n","sequence_length = 50  # Maximum sequence length\n","batch_size = 8\n","epochs = 3\n","learning_rate = 2e-5\n","\n","# Load BERT model and tokenizer\n","bert_model_name = 'bert-base-uncased'\n","bert_model = TFBertModel.from_pretrained(bert_model_name)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","\n","# data preparation\n","X_texts = [sentence[0] for sentence in sentences]\n","y_ner = y_ner.numpy() if isinstance(y_ner, tf.Tensor) else y_ner\n","y_pos = y_pos.numpy() if isinstance(y_pos, tf.Tensor) else y_pos\n","\n","# Tokenize and prepare input data\n","inputs = tokenizer(X_texts, padding=True, truncation=True, max_length=sequence_length, return_tensors='tf')\n","\n","# Extract the input tensors from the dictionary\n","input_ids = inputs['input_ids'].numpy()\n","attention_mask = inputs['attention_mask'].numpy()\n","\n","# Split data into train, validation, and test sets\n","X_train, X_temp, y_train_ner, y_temp_ner = train_test_split(input_ids, y_ner, test_size=0.4, random_state=42)\n","X_val, X_test, y_val_ner, y_test_ner = train_test_split(X_temp, y_temp_ner, test_size=0.5, random_state=42)\n","\n","# Split POS labels similarly\n","#y_train_pos, y_val_pos, y_test_pos = train_test_split(y_pos, test_size=0.4, random_state=42)\n","#y_val_pos, y_test_pos = train_test_split(y_val_pos, test_size=0.5, random_state=42)\n","\n","# First, split into train and temp (which will later be split into val and test)\n","y_train_pos, y_temp_pos = train_test_split(y_pos, test_size=0.4, random_state=42)\n","\n","# Then split the temp set into validation and test sets\n","y_val_pos, y_test_pos = train_test_split(y_temp_pos, test_size=0.5, random_state=42)\n","\n","# Define the model\n","class MultiTaskModel(tf.keras.Model):\n","    def __init__(self, ner_classes, pos_classes):\n","        super(MultiTaskModel, self).__init__()\n","        self.bert = bert_model\n","        self.dense = Dense(hidden_dim, activation='relu')\n","        self.ner_crf = tfa.layers.CRF(ner_classes)\n","        self.pos_crf = tfa.layers.CRF(pos_classes)\n","\n","    def call(self, inputs, attention_mask):\n","        bert_output = self.bert(inputs, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        features = self.dense(sequence_output)\n","        ner_output = self.ner_crf(features)\n","        pos_output = self.pos_crf(features)\n","        return ner_output, pos_output\n","\n","def crf_loss(y_true, y_pred):\n","    log_likelihood, transition_params = tfa.text.crf.crf_log_likelihood(\n","        y_pred, y_true, tf.math.count_nonzero(y_pred, axis=1)\n","    )\n","    return -tf.reduce_mean(log_likelihood)\n","model = MultiTaskModel(ner_classes, pos_classes)\n","model.compile(optimizer=Adam(learning_rate=learning_rate),\n","              loss={'crf_output_1': crf_loss, 'crf_output_2': crf_loss},\n","              metrics={'crf_output_1': 'accuracy', 'crf_output_2': 'accuracy'})\n","\n","#model.summary()\n","\n","# Train the model\n","history = model.fit(\n","    {'input_ids': X_train, 'attention_mask': attention_mask[:len(X_train)]},\n","    {'ner_crf': y_train_ner, 'pos_crf': y_train_pos},\n","    validation_data=(\n","        {'input_ids': X_val, 'attention_mask': attention_mask[len(X_train):len(X_train) + len(X_val)]},\n","        {'ner_crf': y_val_ner, 'pos_crf': y_val_pos}),\n","    epochs=epochs,\n","    batch_size=batch_size\n",")\n","\n","# Evaluate the model\n","eval_results = model.evaluate(\n","    {'input_ids': X_test, 'attention_mask': attention_mask[len(X_train) + len(X_val):]},\n","    {'ner_crf': y_test_ner, 'pos_crf': y_test_pos}\n",")\n","\n","print(f\"Test Loss (NER): {eval_results[1]}\")\n","print(f\"Test Accuracy (NER): {eval_results[3]}\")\n","print(f\"Test Loss (POS): {eval_results[2]}\")\n","print(f\"Test Accuracy (POS): {eval_results[4]}\")\n","\n","# Generate predictions\n","ner_predictions, pos_predictions = model.predict({'input_ids': X_test, 'attention_mask': attention_mask[len(X_train) + len(X_val):]})\n","\n","# Convert logits to predicted labels\n","ner_pred_labels = np.argmax(ner_predictions, axis=-1)\n","pos_pred_labels = np.argmax(pos_predictions, axis=-1)\n","\n","# Flatten the labels for confusion matrix\n","def flatten_labels(labels):\n","    return [label for sublist in labels for label in sublist]\n","\n","y_test_flat_ner = flatten_labels(y_test_ner)\n","y_pred_flat_ner = flatten_labels(ner_pred_labels)\n","\n","# Compute confusion matrix for NER\n","conf_matrix_ner = confusion_matrix(y_test_flat_ner, y_pred_flat_ner)\n","\n","# Plot confusion matrix for NER\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(conf_matrix_ner, annot=True, fmt='d', cmap='Blues', xticklabels=range(ner_classes), yticklabels=range(ner_classes))\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix for NER')\n","plt.show()\n","\n","# Generate classification report for NER\n","report_ner = classification_report(y_test_flat_ner, y_pred_flat_ner, labels=range(ner_classes))\n","print(\"NER Classification Report:\")\n","print(report_ner)\n","\n","# Similarly, compute and plot confusion matrix and classification report for POS\n","y_test_flat_pos = flatten_labels(y_test_pos)\n","y_pred_flat_pos = flatten_labels(pos_pred_labels)\n","\n","conf_matrix_pos = confusion_matrix(y_test_flat_pos, y_pred_flat_pos)\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(conf_matrix_pos, annot=True, fmt='d', cmap='Blues', xticklabels=range(pos_classes), yticklabels=range(pos_classes))\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title('Confusion Matrix for POS')\n","plt.show()\n","\n","report_pos = classification_report(y_test_flat_pos, y_pred_flat_pos, labels=range(pos_classes))\n","print(\"POS Classification Report:\")\n","print(report_pos)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":550},"id":"K8LAmhw8RirJ","outputId":"3008c312-06c7-441a-971c-6c7bb0a75708"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['attention_mask'].","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6f47babea596>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'ner_crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_ner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train_pos\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   3486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositional_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3488\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3489\u001b[0m                 \u001b[0;34mf\"Models passed to `{method_name}` can only have `training` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3490\u001b[0m                 \u001b[0;34m\"and the first argument in `call()` as positional arguments, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['attention_mask']."]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n","from transformers import TFBertModel, BertTokenizer\n","\n","# Hyperparameters\n","vocab_size = 10000  # Example vocabulary size\n","embedding_dim = 128\n","hidden_dim = 256\n","ner_classes = 10  # Number of NER classes\n","pos_classes = 20  # Number of POS classes\n","sequence_length = 50  # Maximum sequence length\n","learning_rate = 0.001\n","\n","# Load pre-trained BERT model\n","bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n","\n","# Define the multi-task model\n","class MultiTaskModel(tf.keras.Model):\n","    def __init__(self, ner_classes, pos_classes):\n","        super(MultiTaskModel, self).__init__()\n","        self.bert = bert_model\n","        self.dense = tf.keras.layers.Dense(hidden_dim, activation='relu')\n","        self.ner_crf = tfa.layers.CRF(ner_classes)\n","        self.pos_crf = tfa.layers.CRF(pos_classes)\n","\n","    def call(self, inputs):\n","        input_ids = inputs['input_ids']\n","        attention_mask = inputs['attention_mask']\n","\n","        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = bert_output.last_hidden_state\n","        features = self.dense(sequence_output)\n","        ner_output = self.ner_crf(features)\n","        pos_output = self.pos_crf(features)\n","        return {'ner_crf': ner_output, 'pos_crf': pos_output}\n","\n","# Instantiate the model\n","model = MultiTaskModel(ner_classes, pos_classes)\n","\n","# Define loss functions for NER and POS using the CRF built-in loss\n","losses = {\n","    'ner_crf': model.ner_crf.get_loss,\n","    'pos_crf': model.pos_crf.get_loss\n","}\n","\n","# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss=losses,\n","              metrics={'ner_crf': 'accuracy', 'pos_crf': 'accuracy'})\n","\n","# Example input data (replace with actual data)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","X_texts = [\"Sentence one.\", \"Sentence two.\", \"Another sentence.\"]\n","inputs = tokenizer(X_texts, padding=True, truncation=True, max_length=sequence_length, return_tensors='tf')\n","\n","# Example labels (replace with actual labels)\n","y_ner = tf.random.uniform((len(X_texts), sequence_length), maxval=ner_classes, dtype=tf.int32)\n","y_pos = tf.random.uniform((len(X_texts), sequence_length), maxval=pos_classes, dtype=tf.int32)\n","\n","# Train the model (replace with actual training data and labels)\n","model.fit({'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},\n","          {'ner_crf': y_ner, 'pos_crf': y_pos},\n","          epochs=5, batch_size=32)\n","\n","# Evaluate the model (replace with actual test data and labels)\n","model.evaluate({'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']},\n","               {'ner_crf': y_ner, 'pos_crf': y_pos})\n"],"metadata":{"id":"IeNRSTtvTMJh"},"execution_count":null,"outputs":[]}]}